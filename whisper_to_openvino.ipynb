{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"][\"array\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from pathlib import Path\n",
    "model_name = \"small.en\" # 重い場合はbase.en、余裕がある場合はmedium.en以上\n",
    "\n",
    "model = whisper.load_model(model_name, \"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(sample,verbose=True,language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHISPER_ENCODER_OV = Path(f\"models/whisper-{model_name}/whisper_{model_name}_encoder.xml\")\n",
    "WHISPER_DECODER_OV = Path(f\"models/whisper-{model_name}/whisper_{model_name}_decoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openvino as ov\n",
    "\n",
    "mel = torch.zeros((1, 80 if 'v3' not in model_name else 128, 3000))\n",
    "audio_features = model.encoder(mel)\n",
    "if not WHISPER_ENCODER_OV.exists():\n",
    "    encoder_model = ov.convert_model(model.encoder, example_input=mel)\n",
    "    ov.save_model(encoder_model, WHISPER_ENCODER_OV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def attention_forward(\n",
    "        attention_module,\n",
    "        x: torch.Tensor,\n",
    "        xa: Optional[torch.Tensor] = None,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Override for forward method of decoder attention module with storing cache values explicitly.\n",
    "    Parameters:\n",
    "      attention_module: current attention module\n",
    "      x: input token ids.\n",
    "      xa: input audio features (Optional).\n",
    "      mask: mask for applying attention (Optional).\n",
    "      kv_cache: dictionary with cached key values for attention modules.\n",
    "      idx: idx for search in kv_cache.\n",
    "    Returns:\n",
    "      attention module output tensor\n",
    "      updated kv_cache\n",
    "    \"\"\"\n",
    "    q = attention_module.query(x)\n",
    "\n",
    "    if xa is None:\n",
    "        # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n",
    "        # otherwise, perform key/value projections for self- or cross-attention as usual.\n",
    "        k = attention_module.key(x)\n",
    "        v = attention_module.value(x)\n",
    "        if kv_cache is not None:\n",
    "            k = torch.cat((kv_cache[0], k), dim=1)\n",
    "            v = torch.cat((kv_cache[1], v), dim=1)\n",
    "        kv_cache_new = (k, v)\n",
    "    else:\n",
    "        # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n",
    "        k = attention_module.key(xa)\n",
    "        v = attention_module.value(xa)\n",
    "        kv_cache_new = (None, None)\n",
    "\n",
    "    wv, qk = attention_module.qkv_attention(q, k, v, mask)\n",
    "    return attention_module.out(wv), kv_cache_new\n",
    "\n",
    "\n",
    "def block_forward(\n",
    "    residual_block,\n",
    "    x: torch.Tensor,\n",
    "    xa: Optional[torch.Tensor] = None,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    kv_cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Override for residual block forward method for providing kv_cache to attention module.\n",
    "      Parameters:\n",
    "        residual_block: current residual block.\n",
    "        x: input token_ids.\n",
    "        xa: input audio features (Optional).\n",
    "        mask: attention mask (Optional).\n",
    "        kv_cache: cache for storing attention key values.\n",
    "      Returns:\n",
    "        x: residual block output\n",
    "        kv_cache: updated kv_cache\n",
    "\n",
    "    \"\"\"\n",
    "    x0, kv_cache = residual_block.attn(residual_block.attn_ln(\n",
    "        x), mask=mask, kv_cache=kv_cache)\n",
    "    x = x + x0\n",
    "    if residual_block.cross_attn:\n",
    "        x1, _ = residual_block.cross_attn(\n",
    "            residual_block.cross_attn_ln(x), xa)\n",
    "        x = x + x1\n",
    "    x = x + residual_block.mlp(residual_block.mlp_ln(x))\n",
    "    return x, kv_cache\n",
    "\n",
    "\n",
    "\n",
    "# update forward functions\n",
    "for idx, block in enumerate(model.decoder.blocks):\n",
    "    block.forward = partial(block_forward, block)\n",
    "    block.attn.forward = partial(attention_forward, block.attn)\n",
    "    if block.cross_attn:\n",
    "        block.cross_attn.forward = partial(attention_forward, block.cross_attn)\n",
    "\n",
    "\n",
    "def decoder_forward(decoder, x: torch.Tensor, xa: torch.Tensor, kv_cache: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor]]] = None):\n",
    "    \"\"\"\n",
    "    Override for decoder forward method.\n",
    "    Parameters:\n",
    "      x: torch.LongTensor, shape = (batch_size, <= n_ctx) the text tokens\n",
    "      xa: torch.Tensor, shape = (batch_size, n_mels, n_audio_ctx)\n",
    "           the encoded audio features to be attended on\n",
    "      kv_cache: Dict[str, torch.Tensor], attention modules hidden states cache from previous steps\n",
    "    \"\"\"\n",
    "    if kv_cache is not None:\n",
    "        offset = kv_cache[0][0].shape[1]\n",
    "    else:\n",
    "        offset = 0\n",
    "        kv_cache = [None for _ in range(len(decoder.blocks))]\n",
    "    x = decoder.token_embedding(\n",
    "        x) + decoder.positional_embedding[offset: offset + x.shape[-1]]\n",
    "    x = x.to(xa.dtype)\n",
    "    kv_cache_upd = []\n",
    "\n",
    "    for block, kv_block_cache in zip(decoder.blocks, kv_cache):\n",
    "        x, kv_block_cache_upd = block(x, xa, mask=decoder.mask, kv_cache=kv_block_cache)\n",
    "        kv_cache_upd.append(tuple(kv_block_cache_upd))\n",
    "\n",
    "    x = decoder.ln(x)\n",
    "    logits = (\n",
    "        x @ torch.transpose(decoder.token_embedding.weight.to(x.dtype), 1, 0)).float()\n",
    "\n",
    "    return logits, tuple(kv_cache_upd)\n",
    "\n",
    "# override decoder forward\n",
    "model.decoder.forward = partial(decoder_forward, model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.ones((5, 3), dtype=torch.int64)\n",
    "logits, kv_cache = model.decoder(tokens, audio_features, kv_cache=None)\n",
    "\n",
    "tokens = torch.ones((5, 1), dtype=torch.int64)\n",
    "\n",
    "if not WHISPER_DECODER_OV.exists():\n",
    "    decoder_model = ov.convert_model(model.decoder, example_input=(tokens, audio_features, kv_cache))\n",
    "    ov.save_model(decoder_model, WHISPER_DECODER_OV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import patch_whisper_for_ov_inference, OpenVINOAudioEncoder, OpenVINOTextDecoder\n",
    "\n",
    "patch_whisper_for_ov_inference(model)\n",
    "core = ov.Core()\n",
    "device = \"GPU\"\n",
    "model.encoder = OpenVINOAudioEncoder(core, WHISPER_ENCODER_OV, device=device)\n",
    "model.decoder = OpenVINOTextDecoder(core, WHISPER_DECODER_OV, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(sample,verbose=True,language=\"en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TRSVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
