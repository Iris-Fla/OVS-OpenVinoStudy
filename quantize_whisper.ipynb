{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_name = \"small.en\"\n",
    "WHISPER_ENCODER_OV = Path(f\"models/whisper-{model_name}/whisper_{model_name}_encoder.xml\")\n",
    "WHISPER_DECODER_OV = Path(f\"models/whisper-{model_name}/whisper_{model_name}_decoder.xml\")\n",
    "\n",
    "WHISPER_ENCODER_OV_INT8 = Path(f\"models/whisper-{model_name}/whisper_{model_name}_encoder_int8.xml\")\n",
    "WHISPER_DECODER_OV_INT8 = Path(f\"models/whisper-{model_name}/whisper_{model_name}_decoder_int8.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import whisper\n",
    "from util import patch_whisper_for_ov_inference, OpenVINOAudioEncoder, OpenVINOTextDecoder\n",
    "\n",
    "core = ov.Core()\n",
    "device=\"CPU\"\n",
    "model_fp32 = whisper.load_model(model_name, \"cpu\").eval()\n",
    "patch_whisper_for_ov_inference(model_fp32)\n",
    "\n",
    "model_fp32.encoder = OpenVINOAudioEncoder(core, WHISPER_ENCODER_OV, device=device)\n",
    "model_fp32.decoder = OpenVINOTextDecoder(core, WHISPER_DECODER_OV, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "COLLECT_CALIBRATION_DATA = False\n",
    "encoder_calibration_data = []\n",
    "decoder_calibration_data = []\n",
    "\n",
    "@contextmanager\n",
    "def calibration_data_collection():\n",
    "    global COLLECT_CALIBRATION_DATA\n",
    "    try:\n",
    "        COLLECT_CALIBRATION_DATA = True\n",
    "        yield\n",
    "    finally:\n",
    "        COLLECT_CALIBRATION_DATA = False\n",
    "\n",
    "\n",
    "def encoder_forward(self, mel: torch.Tensor):\n",
    "    if COLLECT_CALIBRATION_DATA:\n",
    "        encoder_calibration_data.append(mel)\n",
    "    return torch.from_numpy(self.compiled_model(mel)[self.output_blob])\n",
    "\n",
    "def decoder_forward(self, x: torch.Tensor, xa: torch.Tensor, kv_cache: Optional[dict] = None):\n",
    "    feed_dict = {'x': ov.Tensor(x.numpy()), 'xa': ov.Tensor(xa.numpy())}\n",
    "    feed_dict = (self.preprocess_kv_cache_inputs(feed_dict, kv_cache))\n",
    "    if COLLECT_CALIBRATION_DATA:\n",
    "        decoder_calibration_data.append(feed_dict)\n",
    "    res = self.compiled_model(feed_dict)\n",
    "    return self.postprocess_outputs(res)\n",
    "\n",
    "model_fp32.encoder.forward = partial(encoder_forward, model_fp32.encoder)\n",
    "model_fp32.decoder.forward = partial(decoder_forward, model_fp32.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dc60005fc64ee8937091ab42b79536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/11.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d9944f7bd54371b389402b0618a777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting calibration data: 100%|██████████| 30/30 [02:09<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "CALIBRATION_DATASET_SIZE = 30\n",
    "\n",
    "calibration_dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True).take(CALIBRATION_DATASET_SIZE)\n",
    "\n",
    "with calibration_data_collection():\n",
    "    for data_item in tqdm(calibration_dataset, desc=\"Collecting calibration data\", total=CALIBRATION_DATASET_SIZE):\n",
    "        model_fp32.transcribe(data_item[\"audio\"][\"array\"].astype(\"float32\"), task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n",
      "Quantizing encoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529fbe0bfac540aeab06fe210f952ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc8db6e7c254483bbaecf6886042ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:36 ignored nodes were found by name in the NNCFGraph\n",
      "INFO:nncf:48 ignored nodes were found by name in the NNCFGraph\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9c9e5a00a7437b9ce0cf6ffdff97c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7c4e40914a462f803b99459bb68b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nncf\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "print(\"Quantizing encoder...\")\n",
    "quantized_encoder = nncf.quantize(\n",
    "    model=model_fp32.encoder.model,\n",
    "    calibration_dataset=nncf.Dataset(encoder_calibration_data),\n",
    "    subset_size=len(encoder_calibration_data),\n",
    "    model_type=nncf.ModelType.TRANSFORMER,\n",
    "    advanced_parameters=nncf.AdvancedQuantizationParameters(\n",
    "        smooth_quant_alpha=0.5      # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n",
    "    ),\n",
    "    target_device=nncf.TargetDevice.GPU # GPUで推論する場合必要\n",
    ")\n",
    "serialize(quantized_encoder, WHISPER_ENCODER_OV_INT8)\n",
    "print(f\"Saved quantized encoder at ./{WHISPER_ENCODER_OV_INT8}\")\n",
    "\n",
    "print(\"Quantizing decoder...\")\n",
    "quantized_decoder = nncf.quantize(\n",
    "    model=model_fp32.decoder.model,\n",
    "    calibration_dataset=nncf.Dataset(decoder_calibration_data),\n",
    "    subset_size=len(decoder_calibration_data),\n",
    "    model_type=nncf.ModelType.TRANSFORMER,\n",
    "    advanced_parameters=nncf.AdvancedQuantizationParameters(\n",
    "        smooth_quant_alpha=0.95     # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n",
    "    ),\n",
    "    target_device=nncf.TargetDevice.GPU # GPUで推論する場合必要\n",
    ")\n",
    "serialize(quantized_decoder, WHISPER_DECODER_OV_INT8)\n",
    "print(f\"Saved quantized decoder at ./{WHISPER_DECODER_OV_INT8}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "device = \"GPU\"\n",
    "# from utils import patch_whisper_for_ov_inference, OpenVINOAudioEncoder, OpenVINOTextDecoder\n",
    "model = whisper.load_model(model_name, \"cpu\").eval()\n",
    "patch_whisper_for_ov_inference(model)\n",
    "\n",
    "model.encoder = OpenVINOAudioEncoder(core, WHISPER_ENCODER_OV, device=device)\n",
    "model.decoder = OpenVINOTextDecoder(core, WHISPER_DECODER_OV, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "sample = ds[0][\"audio\"][\"array\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transcribe(sample,verbose=True,language=\"en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
